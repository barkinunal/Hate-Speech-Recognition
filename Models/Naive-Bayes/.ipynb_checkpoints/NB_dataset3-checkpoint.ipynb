{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "h_X3xJ49i78C",
    "outputId": "c0cbfb05-6087-4f9a-ce1b-7f8206248446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l0lPPKU9jAzo"
   },
   "outputs": [],
   "source": [
    "__author__ = \"Gökhan Özeloğlu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ggbdmgXhjPDA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1bGnp6qjUdd"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/ML_Term_Project/\"\n",
    "data = pd.read_csv(path + \"merged_and_cleaned.csv\")\n",
    "data_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "5x5vCRcbjsTa",
    "outputId": "611cf0b5-8a57-44a5-ac84-d8dc461b837f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TIL I'm the district attorney.</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Aw there's nothing to cry about Lynn xx</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Call me sexist, but shouldn't everyone on the ...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>\"We fought so hard in this entire comp it woul...</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>This season is going to go on forever</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index  ...   class norm\n",
       "0           0      0  ...    none    2\n",
       "1           1      1  ...    none    2\n",
       "2           2      2  ...  sexism    1\n",
       "3           3      3  ...    none    2\n",
       "4           4      4  ...    none    2\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTqAcj4Bjv0P"
   },
   "outputs": [],
   "source": [
    "# Some useless columns are dropped\n",
    "data_df = data_df.drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "B16BmvPtj2Bw",
    "outputId": "fbeb3510-35ce-4c49-aeec-99884f142eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16907 entries, 0 to 16906\n",
      "Data columns (total 4 columns):\n",
      "index    16907 non-null int64\n",
      "text     16595 non-null object\n",
      "class    16907 non-null object\n",
      "norm     16907 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 528.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "qVDno1egj7Qh",
    "outputId": "de921cae-a6c9-4f54-f722-2f886026ce05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0        2\n",
       " 1        2\n",
       " 2        1\n",
       " 3        2\n",
       " 4        2\n",
       "         ..\n",
       " 16902    1\n",
       " 16903    2\n",
       " 16904    2\n",
       " 16905    2\n",
       " 16906    2\n",
       " Name: norm, Length: 16907, dtype: int64,\n",
       "        index                                               text   class  norm\n",
       " 16867  16867  Palestinians accepted the 2state solution.one ...  racism     0\n",
       " 16868  16868  ISIS, Boko Haram, Al Queda, Taliban, Iran hang...    none     2\n",
       " 16869  16869          bahah google suggestions knows what's up.    none     2\n",
       " 16870  16870  I'm not sexist but women sort out ur fucking p...  sexism     1\n",
       " 16871  16871                                                yup    none     2\n",
       " 16872  16872            3, 4, 4 from me. Below average all over    none     2\n",
       " 16873  16873                           Almost time to serve...!    none     2\n",
       " 16874  16874                               I have a sunburn. :(    none     2\n",
       " 16875  16875  One of the vile things about Islam is that it ...  racism     0\n",
       " 16876  16876  The only thing that seems to be screwing up to...    none     2\n",
       " 16877  16877  Already did. Found out that Islam is a vile, b...  racism     0\n",
       " 16878  16878  Missing Person: Dolores Dennis (81, 5'5', 100l...    none     2\n",
       " 16879  16879     I think maybe the \"hat\" is turning people off.    none     2\n",
       " 16880  16880                           Sick of the word Sassy!!    none     2\n",
       " 16881  16881  ruby seems like it would have a lot of stuff t...    none     2\n",
       " 16882  16882                                      Kat is a mole    none     2\n",
       " 16883  16883                                     3 chef wankers    none     2\n",
       " 16884  16884  compared to the sewer and violence and poverty...  racism     0\n",
       " 16885  16885  We’ve just said goodbye to another team! Are y...    none     2\n",
       " 16886  16886  Before CNN refrained from showing the Charlie ...    none     2\n",
       " 16887  16887  why do all these people WANT to cook for a liv...  sexism     1\n",
       " 16888  16888                                           YESSSSSS    none     2\n",
       " 16889  16889  Go ahead and remain a moron. Makes no differen...    none     2\n",
       " 16890  16890  that is horrible. :( i've only heard about thi...    none     2\n",
       " 16891  16891  also (the awesome) counted the of women he fol...    none     2\n",
       " 16892  16892  And here you are honoring terrorists that your...    none     2\n",
       " 16893  16893  Those kisses on the mirror look like. ....cats...    none     2\n",
       " 16894  16894  Has anyone made a Twitter account for Rob's ha...    none     2\n",
       " 16895  16895  it's true, there's a risk. but the same risk i...    none     2\n",
       " 16896  16896  Have you ever had a female judge outraged at y...  sexism     1\n",
       " 16897  16897  Little short and to the point ISLAM Lesson's f...    none     2\n",
       " 16898  16898  No, it's not SEXIST...But I do have a theory t...  sexism     1\n",
       " 16899  16899  That's a huge serve of pancakes!!! Kids would ...    none     2\n",
       " 16900  16900  ...whereas, prostitution is the same job for s...  sexism     1\n",
       " 16901  16901  Not only were there no sanctions against food ...    none     2\n",
       " 16902  16902  ash and camilla have grown on me, but can Kat ...  sexism     1\n",
       " 16903  16903  Kat you've been owned: \"there's only one way t...    none     2\n",
       " 16904  16904  Because if non of those things are true, I don...    none     2\n",
       " 16905  16905                          Why not just make a tart?    none     2\n",
       " 16906  16906  We didn't overturn elections in either place, ...    none     2)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.norm, data_df.tail(40)\n",
    "# 0 --> racism\n",
    "# 1 --> sexism\n",
    "# 2 --> none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTA7z786kXsP"
   },
   "outputs": [],
   "source": [
    "data_df.text = data_df.text.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7060jxajkfih"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@:param documents represents corpus data\n",
    "@:param stop_word means stop words are included or not. Default value is True\n",
    "If you want to include stop words you should give False as a parameter\n",
    "@:param n_gram indicate ngram range of TfidfVectorizer. Default value is \n",
    "(1, 1). Its mean unigram. \n",
    "(2, 2) --> bigram\n",
    "(3, 3) --> 3-gram\n",
    "(4, 4) --> 4-gram\n",
    "@:return a numpy array\n",
    "@:details min_df is determined as 0.0005. It ignores vocabulary if it has a\n",
    "document frequency strictly lower than 0.0005. This value is a threshold value.\n",
    "\"\"\"\n",
    "def bag_word(documents, stop_word=True, n_gram=(1,1)):\n",
    "\n",
    "    # If stop words want to be excluded.\n",
    "    if stop_word:\n",
    "        if n_gram == (1,1):\n",
    "            tfidfconverter = TfidfVectorizer(ngram_range=n_gram, stop_words='english')\n",
    "        else:\n",
    "            tfidfconverter = TfidfVectorizer(ngram_range=n_gram, min_df=0.0005, stop_words='english')\n",
    "    \n",
    "    # If stop words want to be included\n",
    "    if not stop_word:\n",
    "        if n_gram == (1, 1):\n",
    "            tfidfconverter = TfidfVectorizer(ngram_range=n_gram)\n",
    "        else:\n",
    "            tfidfconverter = TfidfVectorizer(ngram_range=n_gram, min_df=0.0005)\n",
    "\n",
    "    X = tfidfconverter.fit_transform(documents).toarray()\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "Em_ym-k4kjqh",
    "outputId": "c618b2de-9d3d-43d0-85d7-aa7b530e3c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  1 - gram without stopwords :  0.7527053591248498\n",
      "MultinomialNB  1 - gram with stopwords :  0.7296379174498268\n",
      "MultinomialNB  2 - gram without stopwords :  0.7360256860346887\n",
      "MultinomialNB  2 - gram with stopwords :  0.766131364212258\n",
      "MultinomialNB  3 - gram without stopwords :  0.6885906374207426\n",
      "MultinomialNB  3 - gram with stopwords :  0.7400484808248954\n",
      "MultinomialNB  4 - gram without stopwords :  0.6836813540418708\n",
      "MultinomialNB  4 - gram with stopwords :  0.7171584836271412\n",
      "MultinomialNB  5 - gram without stopwords :  0.6836813540418708\n",
      "MultinomialNB  5 - gram with stopwords :  0.6872300403887471\n",
      "MultinomialNB  6 - gram without stopwords :  0.6836813540418708\n",
      "MultinomialNB  6 - gram with stopwords :  0.6836813540418708\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for i in range(1, 7):\n",
    "    for exp in [True, False]:\n",
    "        X = data_df['text'].values.astype('U')\n",
    "        y = data_df[\"norm\"].values.astype('U')\n",
    "        X = bag_word(X, stop_word=exp, n_gram=(i, i))\n",
    "        clfrNB = MultinomialNB()\n",
    "        scores = cross_val_score(clfrNB, X, y,  cv=10)\n",
    "        acc = scores.mean()\n",
    "        acc_list.append(acc)\n",
    "        if exp:\n",
    "            print(\"MultinomialNB \", i, \"- gram without stopwords : \", acc) \n",
    "        else:\n",
    "            print(\"MultinomialNB \", i, \"- gram with stopwords : \", acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NB_dataset3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
